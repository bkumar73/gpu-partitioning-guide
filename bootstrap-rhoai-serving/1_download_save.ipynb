{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Save the Model\n",
    "\n",
    "To save this model so that you can use it from various locations, including other notebooks or the model server, upload it to s3-compatible storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the required packages and define a function for the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress only the InsecureRequestWarning from urllib3\n",
    "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/app-root/lib/python3.9/site-packages (1.28.85)\n",
      "Requirement already satisfied: botocore in /opt/app-root/lib/python3.9/site-packages (1.31.85)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from boto3) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.9/site-packages (from botocore) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.9/site-packages (from botocore) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACE_USER=\"rcarrata\"\n",
    "HUGGINGFACE_TOKEN=\"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\"\n",
    "\n",
    "#git_repo = \"https://huggingface.co/ibm/merlinite-7b\"\n",
    "#git_repo = \"https://huggingface.co/ibm/granite-7b-base\"\n",
    "\n",
    "# git_repo = \"https://huggingface.co/instructlab/merlinite-7b-lab\"\n",
    "# git_repo = \"https://huggingface.co/instructlab/granite-7b-lab\"\n",
    "\n",
    "# git_repo = \"https://huggingface.co/google/flan-t5-small\"\n",
    "\n",
    "# git_repo = \"https://huggingface.co/codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "# git_repo = \"https://huggingface.co/mosaicml/mpt-7b-chat\"\n",
    "# git_repo = \"https://huggingface.co/mosaicml/mpt-7b-instruct\"\n",
    "\n",
    "#HUGGINGFACE_USER = os.getenv(\"HUGGINGFACE_USER\")\n",
    "#HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "#git_repo = f\"https://{HUGGINGFACE_USER}:{HUGGINGFACE_TOKEN}@huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "git_repo = f\"https://{HUGGINGFACE_TOKEN}:{HUGGINGFACE_TOKEN}@huggingface.co/meta-llama/Llama-2-7b-chat-hf\"\n",
    "# git_repo = f\"https://{HUGGINGFACE_TOKEN}:{HUGGINGFACE_TOKEN}@huggingface.co/meta-llama/Llama-2-13b-chat-hf\"\n",
    "!git config pull.rebase false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Llama-2-7b-chat-hf' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone $git_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2-7b-chat-hf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = os.path.basename(git_repo)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "#upload the model directory without git\n",
    "def upload_directory_to_s3(local_directory, s3_prefix, remove_safetensors=True):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            if \".git\" in relative_path:\n",
    "                print(f\"skipping {relative_path}\")\n",
    "                continue\n",
    "            # if remove_safetensors and \".safetensors\" in relative_path:\n",
    "            #     print(f\"skipping {relative_path}\")\n",
    "            #     continue\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)\n",
    "        \n",
    "def delete_subdirectory(s3_prefix):\n",
    "    \"\"\"\n",
    "    Deletes all objects under a specified S3 prefix, effectively removing the subdirectory.\n",
    "    \"\"\"\n",
    "    objects_to_delete = bucket.objects.filter(Prefix=s3_prefix)\n",
    "    delete_keys = {'Objects': [{'Key': obj.key} for obj in objects_to_delete]}\n",
    "    if delete_keys['Objects']:\n",
    "        bucket.delete_objects(Delete=delete_keys)\n",
    "        print(f\"Deleted all objects in directory {s3_prefix}\")\n",
    "    else:\n",
    "        print(f\"No objects found in directory {s3_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "delete_subdirectory('models/granite-7b-base/')\n",
    "## Check the Storage Bucket\n",
    "\n",
    "In your S3 bucket, under the `models` upload prefix, run the `list_object` command. As best practice, to avoid mixing up model files, keep only one model and its required files in a given prefix or directory. This practice allows you to download and serve a directory with all the files that a model requires. \n",
    "\n",
    "If this is the first time running the code, this cell will have no output or the fraud model from the predictive AI/ML exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Mistral-7B-Instruct-v0.2/README.md\n",
      "models/Mistral-7B-Instruct-v0.2/config.json\n",
      "models/Mistral-7B-Instruct-v0.2/generation_config.json\n",
      "models/Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model-00002-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model-00003-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model.safetensors.index.json\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00001-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00002-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00003-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json\n",
      "models/Mistral-7B-Instruct-v0.2/special_tokens_map.json\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer.json\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer.model\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer_config.json\n",
      "models/fraud/1/model.onnx\n",
      "models/granite-7b-base/README.md\n",
      "models/granite-7b-base/config.json\n",
      "models/granite-7b-base/generation_config.json\n",
      "models/granite-7b-base/model-00001-of-00006.safetensors\n",
      "models/granite-7b-base/model-00002-of-00006.safetensors\n",
      "models/granite-7b-base/model-00003-of-00006.safetensors\n",
      "models/granite-7b-base/model-00004-of-00006.safetensors\n",
      "models/granite-7b-base/model-00005-of-00006.safetensors\n",
      "models/granite-7b-base/model-00006-of-00006.safetensors\n",
      "models/granite-7b-base/model.safetensors.index.json\n",
      "models/granite-7b-base/special_tokens_map.json\n",
      "models/granite-7b-base/tokenizer.json\n",
      "models/granite-7b-base/tokenizer.model\n",
      "models/granite-7b-base/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload and check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to upload the `models` folder in a rescursive fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b-chat-hf/README.md -> models/Llama-2-7b-chat-hf/README.md\n",
      "skipping .gitattributes\n",
      "Llama-2-7b-chat-hf/tokenizer.json -> models/Llama-2-7b-chat-hf/tokenizer.json\n",
      "Llama-2-7b-chat-hf/config.json -> models/Llama-2-7b-chat-hf/config.json\n",
      "Llama-2-7b-chat-hf/generation_config.json -> models/Llama-2-7b-chat-hf/generation_config.json\n",
      "Llama-2-7b-chat-hf/pytorch_model-00002-of-00002.bin -> models/Llama-2-7b-chat-hf/pytorch_model-00002-of-00002.bin\n",
      "Llama-2-7b-chat-hf/tokenizer.model -> models/Llama-2-7b-chat-hf/tokenizer.model\n",
      "Llama-2-7b-chat-hf/model.safetensors.index.json -> models/Llama-2-7b-chat-hf/model.safetensors.index.json\n",
      "Llama-2-7b-chat-hf/pytorch_model.bin.index.json -> models/Llama-2-7b-chat-hf/pytorch_model.bin.index.json\n",
      "Llama-2-7b-chat-hf/model-00002-of-00002.safetensors -> models/Llama-2-7b-chat-hf/model-00002-of-00002.safetensors\n",
      "Llama-2-7b-chat-hf/LICENSE.txt -> models/Llama-2-7b-chat-hf/LICENSE.txt\n",
      "Llama-2-7b-chat-hf/tokenizer_config.json -> models/Llama-2-7b-chat-hf/tokenizer_config.json\n",
      "Llama-2-7b-chat-hf/special_tokens_map.json -> models/Llama-2-7b-chat-hf/special_tokens_map.json\n",
      "Llama-2-7b-chat-hf/USE_POLICY.md -> models/Llama-2-7b-chat-hf/USE_POLICY.md\n",
      "skipping .git/description\n",
      "skipping .git/packed-refs\n",
      "skipping .git/index.lock\n",
      "skipping .git/HEAD\n",
      "skipping .git/config\n",
      "skipping .git/logs/HEAD\n",
      "skipping .git/logs/refs/remotes/origin/HEAD\n",
      "skipping .git/logs/refs/heads/main\n",
      "skipping .git/lfs/objects/9e/55/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
      "skipping .git/lfs/objects/0f/d6/0fd6895090da1b2ccffdb93964847709a3b31e6b69fe7dc5a480dce37c811b1d\n",
      "skipping .git/lfs/objects/1d/e3/1de3167f93dcd7c26ac5ad7077dfa987f2bc18e10078ccca6c975b2825c74827\n",
      "skipping .git/lfs/objects/66/de/66dec18c9f1705b9387d62f8485f4e7d871ca388718786737ed3c72dbfaac9fb\n",
      "skipping .git/lfs/objects/aa/5e/aa5ed69a0bc6fb6c520651607f0d857f2aac245e721087d706926cf032887342\n",
      "skipping .git/refs/remotes/origin/HEAD\n",
      "skipping .git/refs/heads/main\n",
      "skipping .git/info/exclude\n",
      "skipping .git/objects/pack/pack-1c1a12800f3acfc727ab3f5a48fa2ed2fcb46872.idx\n",
      "skipping .git/objects/pack/pack-1c1a12800f3acfc727ab3f5a48fa2ed2fcb46872.pack\n",
      "skipping .git/hooks/applypatch-msg.sample\n",
      "skipping .git/hooks/prepare-commit-msg.sample\n",
      "skipping .git/hooks/fsmonitor-watchman.sample\n",
      "skipping .git/hooks/pre-rebase.sample\n",
      "skipping .git/hooks/pre-applypatch.sample\n",
      "skipping .git/hooks/pre-commit.sample\n",
      "skipping .git/hooks/post-update.sample\n",
      "skipping .git/hooks/post-merge\n",
      "skipping .git/hooks/pre-receive.sample\n",
      "skipping .git/hooks/push-to-checkout.sample\n",
      "skipping .git/hooks/post-checkout\n",
      "skipping .git/hooks/commit-msg.sample\n",
      "skipping .git/hooks/post-commit\n",
      "skipping .git/hooks/pre-merge-commit.sample\n",
      "skipping .git/hooks/update.sample\n",
      "skipping .git/hooks/pre-push\n",
      "skipping .git/hooks/pre-push.sample\n"
     ]
    }
   ],
   "source": [
    "upload_directory_to_s3(model_name, f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To confirm this worked, run the `list_objects` function again:\n",
    "\n",
    "This time, you should see files listed in the directory/prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Llama-2-7b-chat-hf/LICENSE.txt\n",
      "models/Llama-2-7b-chat-hf/README.md\n",
      "models/Llama-2-7b-chat-hf/USE_POLICY.md\n",
      "models/Llama-2-7b-chat-hf/config.json\n",
      "models/Llama-2-7b-chat-hf/generation_config.json\n",
      "models/Llama-2-7b-chat-hf/model-00002-of-00002.safetensors\n",
      "models/Llama-2-7b-chat-hf/model.safetensors.index.json\n",
      "models/Llama-2-7b-chat-hf/pytorch_model-00002-of-00002.bin\n",
      "models/Llama-2-7b-chat-hf/pytorch_model.bin.index.json\n",
      "models/Llama-2-7b-chat-hf/special_tokens_map.json\n",
      "models/Llama-2-7b-chat-hf/tokenizer.json\n",
      "models/Llama-2-7b-chat-hf/tokenizer.model\n",
      "models/Llama-2-7b-chat-hf/tokenizer_config.json\n",
      "models/Mistral-7B-Instruct-v0.2/README.md\n",
      "models/Mistral-7B-Instruct-v0.2/config.json\n",
      "models/Mistral-7B-Instruct-v0.2/generation_config.json\n",
      "models/Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model-00002-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model-00003-of-00003.safetensors\n",
      "models/Mistral-7B-Instruct-v0.2/model.safetensors.index.json\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00001-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00002-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model-00003-of-00003.bin\n",
      "models/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json\n",
      "models/Mistral-7B-Instruct-v0.2/special_tokens_map.json\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer.json\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer.model\n",
      "models/Mistral-7B-Instruct-v0.2/tokenizer_config.json\n",
      "models/fraud/1/model.onnx\n",
      "models/granite-7b-base/README.md\n",
      "models/granite-7b-base/config.json\n",
      "models/granite-7b-base/generation_config.json\n",
      "models/granite-7b-base/model-00001-of-00006.safetensors\n",
      "models/granite-7b-base/model-00002-of-00006.safetensors\n",
      "models/granite-7b-base/model-00003-of-00006.safetensors\n",
      "models/granite-7b-base/model-00004-of-00006.safetensors\n",
      "models/granite-7b-base/model-00005-of-00006.safetensors\n",
      "models/granite-7b-base/model-00006-of-00006.safetensors\n",
      "models/granite-7b-base/model.safetensors.index.json\n",
      "models/granite-7b-base/special_tokens_map.json\n",
      "models/granite-7b-base/tokenizer.json\n",
      "models/granite-7b-base/tokenizer.model\n",
      "models/granite-7b-base/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "Now that you've saved the model to s3 storage, you can refer to the model by using the same data connection to serve the model as an API.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
